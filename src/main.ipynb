{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "\n",
    "\n",
    "class C_Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_dim=128, output_dim=128, dropout=0.2,  n_output=1, dilaSize=1):\n",
    "        \n",
    "        \n",
    "        super(C_Net, self).__init__()\n",
    "\n",
    "        self.embed_smile = nn.Embedding(65, emb_dim)\n",
    "        self.embed_prot = nn.Embedding(26, emb_dim)\n",
    "        \n",
    "        #smiles \n",
    "        self.smiles = nn.Sequential(\n",
    "            nn.Conv1d(in_channels= emb_dim, out_channels= emb_dim, kernel_size=3,padding=dilaSize, dilation=dilaSize),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= emb_dim , out_channels= emb_dim, kernel_size=3,padding=dilaSize * 2 ,dilation=dilaSize * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= emb_dim , out_channels= emb_dim, kernel_size=3,padding=dilaSize * 4 ,dilation=dilaSize * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= emb_dim , out_channels= emb_dim, kernel_size=3,padding=dilaSize * 8 ,dilation=dilaSize * 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= emb_dim , out_channels= emb_dim, kernel_size=3,padding=dilaSize * 16,dilation=dilaSize * 16),\n",
    "            nn.ReLU(),\n",
    "           nn.AdaptiveMaxPool1d(1)                           \n",
    "        )\n",
    "            \n",
    "        #proteins sequence\n",
    "        self.proteins = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=emb_dim, out_channels=emb_dim, kernel_size=3,padding=dilaSize ,dilation=dilaSize),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= emb_dim  , out_channels=emb_dim, kernel_size=3,padding=dilaSize *2 ,dilation=dilaSize *2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= emb_dim , out_channels= emb_dim, kernel_size=3,padding=dilaSize * 4 ,dilation=dilaSize * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= emb_dim , out_channels= emb_dim, kernel_size=3,padding=dilaSize * 8 ,dilation=dilaSize * 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= emb_dim , out_channels= emb_dim, kernel_size=3,padding=dilaSize * 16,dilation=dilaSize * 16),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(1)\n",
    "          )\n",
    "        \n",
    "        \n",
    "        self.layer2 = 16;\n",
    "        self.layer3 =  8;       \n",
    "        \n",
    "\n",
    "        self.predict = nn.Sequential(\n",
    "                                    nn.Linear(2 * output_dim, self.layer2 ), \n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Dropout(dropout),\n",
    "                                    nn.Linear(self.layer2 , self.layer3),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Dropout(dropout),\n",
    "                                    nn.Linear(self.layer3 , n_output)#,\n",
    "                                    )\n",
    "  \n",
    "    def forward(self, smi_in, seq_in,  smi_desc):       \n",
    "        embedded_smi  = self.embed_smile(smi_in) \n",
    "        embedded_seq  = self.embed_prot(seq_in)\n",
    "        \n",
    "        smi = self.smiles(embedded_smi.transpose(1,2))\n",
    "        seq = self.proteins(embedded_seq.transpose(1,2))\n",
    "        \n",
    "        smi = smi.squeeze()\n",
    "        seq = seq.squeeze()\n",
    "        smi_seq = torch.cat((smi, seq),1)          \n",
    "        \n",
    "        out = self.predict(smi_seq)\n",
    "        \n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "import math\n",
    "import pennylane as qml\n",
    "from functools import partial\n",
    "import pennylane.numpy as np\n",
    "\n",
    "\n",
    "\n",
    "qubits  = 8\n",
    "layers  = 3 \n",
    "features = 256\n",
    "#angle_encoding = math.ceil(((features)/qubits)/2) #dense angle\n",
    "angle_encoding = math.ceil(((features)/qubits)) #angle\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=qubits )\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def circuit_amplitude_qml(inputs, weights):\n",
    "\n",
    "    qml.AmplitudeEmbedding(inputs, wires=range(qubits), normalize=True)\n",
    "    qml.StronglyEntanglingLayers(weights, wires = range (qubits), ranges=[1,1,1])   \n",
    "    for i in range(qubits-1):\n",
    "         qml.CNOT(wires=[ (i + 1) % qubits, 0])\n",
    "\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def circuit_angle_qml(inputs, weights):\n",
    "    for i in range(angle_encoding):\n",
    "        qml.AngleEmbedding(math.pi * inputs[:,i*qubits:i*qubits+qubits], wires=range(qubits))    #rotation='X'\n",
    "        qml.StronglyEntanglingLayers(weights[i], wires = range (qubits), ranges=[1,1,1])   \n",
    "\n",
    "    for i in range(qubits-1):\n",
    "         qml.CNOT(wires=[ (i + 1) % qubits, 0])\n",
    "\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def circuit_dense_angle(inputs, weights):                 #inputs  256 (batch size) x 256  output previous layer\n",
    "    \n",
    "    for j in range(angle_encoding):               \n",
    "        for i in range(qubits):                               #eencoding\n",
    "            qml.RX(math.pi *  inputs[:, i*2   + qubits *j] ,  wires=i)      #dense endocoding kai oxi aplo\n",
    "            qml.RY(math.pi *  inputs[:, i*2+1 + qubits *j],   wires=i)\n",
    "\n",
    "        qml.StronglyEntanglingLayers(weights[j], wires = range ( qubits ), ranges=[1,1,1])   \n",
    "\n",
    "    for i in range(qubits-1):\n",
    "         qml.CNOT(wires=[ (i + 1) % qubits, 0])\n",
    "            \n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "\n",
    "class HQ_Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_dim=128,  dropout=0.2,  n_output=1, dilaSize=1, normalize=True):\n",
    "          \n",
    "        super(HQ_Net, self).__init__()\n",
    "\n",
    "        self.embed_smile = nn.Embedding(65, emb_dim)\n",
    "        self.embed_prot = nn.Embedding(26, emb_dim)\n",
    "        \n",
    "        #smiles \n",
    "        self.smiles = nn.Sequential(\n",
    "            nn.Conv1d(in_channels= emb_dim, out_channels= emb_dim, kernel_size=3,padding=dilaSize, dilation=dilaSize),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= emb_dim , out_channels= emb_dim, kernel_size=3,padding=dilaSize * 2 ,dilation=dilaSize * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= emb_dim , out_channels= emb_dim, kernel_size=3,padding=dilaSize * 4 ,dilation=dilaSize * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= emb_dim , out_channels= emb_dim, kernel_size=3,padding=dilaSize * 8 ,dilation=dilaSize * 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= emb_dim , out_channels= emb_dim, kernel_size=3,padding=dilaSize * 16,dilation=dilaSize * 16),\n",
    "            #nn.ReLU(), 7/4/2024 return 0 - 1\n",
    "            nn.Sigmoid(), \n",
    "            nn.AdaptiveMaxPool1d(1)                        \n",
    "        )\n",
    "            \n",
    "        #proteins sequence\n",
    "        self.proteins = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=emb_dim, out_channels=emb_dim, kernel_size=3,padding=dilaSize ,dilation=dilaSize),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= emb_dim  , out_channels=emb_dim, kernel_size=3,padding=dilaSize *2 ,dilation=dilaSize *2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= emb_dim , out_channels= emb_dim, kernel_size=3,padding=dilaSize * 4 ,dilation=dilaSize * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= emb_dim , out_channels= emb_dim, kernel_size=3,padding=dilaSize * 8 ,dilation=dilaSize * 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= emb_dim , out_channels= emb_dim, kernel_size=3,padding=dilaSize * 16,dilation=dilaSize * 16),\n",
    "            #nn.ReLU(), 7/4/2024  7/4/2024 return 0 - 1\n",
    "            nn.Sigmoid(),   \n",
    "            nn.AdaptiveMaxPool1d(1),\n",
    "          )\n",
    "        \n",
    "        #for amplitube\n",
    "        #weight_shapes = {\"weights\": ( layers, qubits, 3)}\n",
    "\n",
    "        # for angle and dense angle\n",
    "        weight_shapes = {\"weights\": (angle_encoding, layers, qubits, 3)} # 3 is for rot\n",
    "\n",
    "        qnode = qml.QNode(circuit_angle_qml, dev, interface='torch', diff_method='backprop') # circuit_amplitude_qml or  circuit_dense_angle\n",
    "        \n",
    "        self.predict_q = qml.qnn.TorchLayer(qnode, weight_shapes) \n",
    "        \n",
    "        #nn.init.uniform_(self.predict_q.weight.data, a=0.0, b=0.01, generator=None) worse perfomance\n",
    "  \n",
    "    def forward(self, smi_in, seq_in,  smi_desc):#65,26\n",
    "\n",
    "        embedded_smi  = self.embed_smile(smi_in) \n",
    "        embedded_seq  = self.embed_prot(seq_in)   \n",
    "        \n",
    "        smi = self.smiles(embedded_smi.transpose(1,2))\n",
    "        seq = self.proteins(embedded_seq.transpose(1,2))\n",
    "\n",
    "        smi = smi.squeeze()\n",
    "        seq = seq.squeeze()\n",
    "\n",
    "        smi_seq =torch.cat((smi, seq) , 1)  \n",
    "        out = self.predict_q(smi_seq)            #quantum\n",
    "        \n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metrics\n",
    "\n",
    "def test(model: nn.Module, test_loader, loss_function, device, show, _p, record):\n",
    "    path = '../run/'\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for idx, (*x, y) in tqdm(enumerate(test_loader), disable=not show, total=len(test_loader)):\n",
    "            for i in range(len(x)):\n",
    "                x[i] = x[i].to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_hat = model(*x)\n",
    "\n",
    "            test_loss += loss_function(y_hat.view(-1), y.view(-1)).item()\n",
    "            outputs.append(y_hat.cpu().numpy().reshape(-1))\n",
    "            targets.append(y.cpu().numpy().reshape(-1))\n",
    "\n",
    "    targets = np.concatenate(targets).reshape(-1)\n",
    "    outputs = np.concatenate(outputs).reshape(-1)\n",
    "\n",
    "    np.savetxt(path + _p + record + 'targets.csv', targets, fmt='%1.2f' )#, fmt ='%d'\n",
    "    np.savetxt(path + _p + record + 'outputs.csv', outputs, fmt='%1.2f')\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    evaluation = {\n",
    "        'loss': test_loss, #einai idio me to MSE\n",
    "        'MSE':  metrics.MSE(targets, outputs),\n",
    "        'RMSE': metrics.RMSE(targets, outputs),\n",
    "        'R2':   metrics.R2(targets, outputs),\n",
    "        #'R2 adjusted':   R2_adjusted(targets, outputs),  # y_train, X_train\n",
    "        'MAE': metrics.MAE(targets, outputs),\n",
    "        'Person': metrics.PERSON(targets, outputs),\n",
    "        'p_value': metrics.P_VALUE(targets, outputs),\n",
    "        'C_INDEX': metrics.C_INDEX(targets, outputs),\n",
    "        'SD': metrics.SD(targets, outputs),\n",
    "    }\n",
    "\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************************* WORKING **************************************\n",
    "##The training code was based on the code of Kaili Wang, 2021 https://github.com/KailiWang1/DeepDTAF#\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pennylane.numpy as np\n",
    "import torch\n",
    "from torch import _pin_memory, nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "from dataset import MyDataset_PDBBind2020, get_scalers_PDBBind2020, MyDataset_pdbbind2016, MyDataset_davis_kiba, get_scalers_davis_kiba\n",
    "\n",
    "print(sys.argv)\n",
    "\n",
    "\n",
    "SHOW_PROCESS_BAR = True\n",
    "\n",
    "\n",
    "seed = np.random.randint(33927, 33928) ##random \n",
    "path = Path(f'../export/{datetime.now().strftime(\"%m%d%H%M\")}_{seed}') \n",
    "\n",
    "output_name= datetime.now().strftime(\"%m%d%H%M\");\n",
    "\n",
    "cuda_name = \"cuda:0\"\n",
    "device = torch.device(cuda_name if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "max_prot_sequence_len = 1000  \n",
    "max_smile_len = 160\n",
    "\n",
    "batch_size = 256\n",
    "epoch = 30  #10, 50 \n",
    "interrupt = None\n",
    "save_best_epoch = 5  # init 5  when `save_best_epoch` is reached and the loss starts to decrease, save best model parameters\n",
    "scale_target = True # For Hybrid montel the target must be scaled \n",
    "scale_inputs = False  \n",
    "\n",
    "\n",
    "# GPU uses cudnn as backend to ensure repeatable by setting the following (in turn, use advances function to speed up training)\n",
    "torch.backends.cudnn.deterministic = False \n",
    "torch.backends.cudnn.benchmark =  True\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "print('path: ', path)\n",
    "\n",
    "writer = SummaryWriter(path)\n",
    "f_param = open(path / 'parameters.txt', 'w')\n",
    "\n",
    "print(f'device = {device}')\n",
    "print(f'seed = {seed}')\n",
    "print(f'write to {path}')\n",
    "print(f'batch_size={batch_size}')\n",
    "print(f'epoch = {epoch}')\n",
    "print(f'Scale target = {scale_target}')\n",
    "print(f'Scale inputs = {scale_inputs}')\n",
    "\n",
    "\n",
    "f_param.write(f'device = {device}\\n'\n",
    "          f'seed = {seed}\\n'\n",
    "          f'write to {path}\\n'\n",
    "          f'batch_size= {batch_size}\\n'\n",
    "          f'epoch = {epoch}\\n'\n",
    "          f'Scale target= {scale_target}\\n'  \n",
    "          f'Scale inputs= {scale_inputs}\\n')\n",
    "\n",
    "\n",
    "assert 0<=save_best_epoch<epoch, 'Save_best_epoch parameter must be greater than epoch '\n",
    "\n",
    "\n",
    "modeling = [C_Net, HQ_Net][1]\n",
    "\n",
    "print('modeling', modeling )\n",
    "\n",
    "\n",
    "print(f'qubits = {qubits}')\n",
    "print(f'layers = {layers}')\n",
    "print(f'blocks = {angle_encoding}')\n",
    "print(f'features = {features}')\n",
    "\n",
    "f_param.write(f'qubits = {qubits}\\n'\n",
    "        f'layers = {layers}\\n'\n",
    "        f'blocks = {angle_encoding}\\n'\n",
    "        f'features = {features}\\n')     \n",
    "     \n",
    "\n",
    "data_name = ['pdbbind2020', 'pdbbind2016', 'davis','kiba'][1]\n",
    "\n",
    "f_param.write(f'data base={data_name}\\n')\n",
    "\n",
    "data_path = '../data/' + data_name\n",
    "print(f'data path to {data_path}')\n",
    "\n",
    "f_param.write(f'data_path={data_path}\\n')\n",
    "\n",
    "model = modeling().to(device)\n",
    "\n",
    "\n",
    "print(model)\n",
    "\n",
    "f_param.write('model: \\n')\n",
    "f_param.write(str(model)+'\\n')\n",
    "#f_param.close()\n",
    "\n",
    "scalers=list()  \n",
    "if data_name == 'pdbbind2020':\n",
    "    scalers = get_scalers_PDBBind2020(data_path, 'training', data_name)\n",
    "    phase_name_array = ['training', 'validation', 'test']\n",
    "    max_smile_len = 160\n",
    "\n",
    "    data_loaders = {phase_name:\n",
    "                    DataLoader(MyDataset_PDBBind2020(data_path, phase_name, data_name, max_prot_sequence_len, max_smile_len, scale_target,\n",
    "                                           scale_inputs, scalers),\n",
    "                                         batch_size=batch_size,\n",
    "                                         pin_memory=True,\n",
    "                                         num_workers=4,\n",
    "                                         shuffle= True)\n",
    "                   for phase_name in phase_name_array}\n",
    "    \n",
    "elif data_name in ['davis', 'kibas']:\n",
    "    scalers = get_scalers_davis_kiba(data_path, 'training', data_name)\n",
    "    phase_name_array = ['training', 'validation', 'test']\n",
    "    if data_name == 'davis':\n",
    "        max_smile_len = 85\n",
    "    else:\n",
    "        max_smile_len = 100\n",
    "\n",
    "    data_loaders = {phase_name:\n",
    "            DataLoader(MyDataset_davis_kiba(data_path, phase_name, data_name, max_prot_sequence_len, max_smile_len, scale_target,\n",
    "                                    scale_inputs, scalers),\n",
    "                                    batch_size=batch_size,\n",
    "                                    pin_memory=True,\n",
    "                                    num_workers=4,\n",
    "                                    shuffle= True)\n",
    "                   for phase_name in phase_name_array}\n",
    "elif data_name == 'pdbbind2016': \n",
    "    print('Scaller in dataset') #scalers = get_scalers(data_path, 'training', data_name)\n",
    "    phase_name_array = ['training', 'validation', 'test', 'test105', 'test71']\n",
    "    max_smile_len = 160\n",
    "\n",
    "    data_loaders = {phase_name:\n",
    "                    DataLoader(MyDataset_pdbbind2016(data_path, phase_name, data_name, max_prot_sequence_len, max_smile_len, scale_target,\n",
    "                                           scale_inputs, scalers),\n",
    "                                         batch_size=batch_size,\n",
    "                                         pin_memory=True,\n",
    "                                         num_workers=4,\n",
    "                                         shuffle= True)\n",
    "                   for phase_name in phase_name_array}\n",
    "else:\n",
    "    print('No dataset')\n",
    "    \n",
    " \n",
    "print(f'max_prot_sequence_len={max_prot_sequence_len}\\n'\n",
    "      f'max_smile_len={max_smile_len}')\n",
    "\n",
    "f_param.write(f'max_prot_sequence_len={max_prot_sequence_len}\\n'\n",
    "      f'max_smile_len={max_smile_len}\\n')\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters()  )\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.0001,\n",
    "                                          epochs=epoch,\n",
    "                                          steps_per_epoch=len(data_loaders['training']))\n",
    "\n",
    "loss_function = nn.MSELoss(reduction='sum')\n",
    "    \n",
    "start = datetime.now()\n",
    "print('start at ', start)\n",
    "\n",
    "best_epoch = -1\n",
    "best_val_loss = 100000000\n",
    "for epoch in range(1, epoch + 1):\n",
    "    tbar = tqdm(enumerate(data_loaders['training']), disable= not SHOW_PROCESS_BAR, total=len(data_loaders['training']))\n",
    "       \n",
    "    for idx, (*x, y) in tbar:\n",
    "        model.train()\n",
    "\n",
    "        for i in range(len(x)):\n",
    "            x[i] = x[i].to(device)\n",
    "\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(*x)\n",
    "       \n",
    "        loss = loss_function(output.view(-1), y.view(-1))  \n",
    "        \n",
    "        loss.backward() \n",
    "            \n",
    "        optimizer.step()\n",
    "        scheduler.step() \n",
    "\n",
    "        tbar.set_description(f' * Train Epoch {epoch} Loss={loss.item() / len(y):.3f}')\n",
    "\n",
    " \n",
    "    for _p in ['training', 'validation']:\n",
    "        performance = test(model, data_loaders[_p], loss_function, device, not SHOW_PROCESS_BAR, _p, record = '_train' + str(epoch) +'_' + output_name) \n",
    "\n",
    "        for i in performance:\n",
    "            writer.add_scalar(f'{i} {_p}', performance[i], global_step=epoch)\n",
    "        if _p=='validation' and epoch>=save_best_epoch and performance['loss']<best_val_loss:\n",
    "            best_val_loss = performance['loss']\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), 'n_best_model.pt')\n",
    "\n",
    "print('best epoch:', best_epoch)\n",
    "f_param.write(f'best epoch={best_epoch}\\n')\n",
    "\n",
    "print('Testing...')\n",
    "\n",
    "model.load_state_dict(torch.load('n_best_model.pt'))\n",
    "with open(path / 'result.txt', 'w') as f:\n",
    "\n",
    "    for _p in phase_name_array:\n",
    "        performance = test(model, data_loaders[_p], loss_function, device,  not SHOW_PROCESS_BAR, _p, record='_test_'+ output_name)\n",
    "        f.write(f'{_p}:\\n')\n",
    "        print(f'{_p}:')\n",
    "        for k, v in performance.items():\n",
    "            f.write(f' {k}: {v}')\n",
    "            print(f' {k}: {v}')\n",
    "        f.write('\\n')\n",
    "        print()\n",
    "\n",
    "writer.close()\n",
    "print('training finished')\n",
    "\n",
    "end = datetime.now()\n",
    "print('end at:', end)\n",
    "print('time used:', str(end - start))\n",
    "\n",
    "f_param.write(f'time used={str(end - start)}\\n')\n",
    "f_param.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pennylane.numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "qubits = 4\n",
    "layers = 5\n",
    "angle_rotations=3\n",
    "features = 8\n",
    "angle_encoding = math.ceil(((features)/8/2))\n",
    "\n",
    "weights_init = 0.01 * np.random.randn(angle_encoding, layers, qubits, angle_rotations, requires_grad=True)\n",
    "inputs =  torch.randn(256, 256)\n",
    "\n",
    "\n",
    "for i in range(angle_encoding):\n",
    "       print(i*qubits, ' - ' , i*qubits+qubits)\n",
    "       tmp = inputs[:,i*qubits:i*qubits+qubits]\n",
    "\n",
    "\n",
    "fig, ax = qml.draw_mpl(circuit_dense_angle)(\n",
    "       inputs,weights_init)\n",
    "plt.show()\n",
    "\n",
    "print(qml.draw(circuit_dense_angle, expansion_strategy=\"device\")(inputs,weights_init))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HQ-DTA39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
