{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://sunitach10.github.io/2019/09/10/DeepDT-in-pytorch.html\n",
    "    \n",
    "https://academic.oup.com/bioinformatics/article/39/2/btad049/6998204"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "errors  []\n"
     ]
    }
   ],
   "source": [
    "#find sequence\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/AMP_SMILES_FROM_FILE.csv', sep =';' )\n",
    "\n",
    "err_prots=[]\n",
    "for i in range(len(df)):\n",
    "    code = df.loc[i, 'PDB code']\n",
    "    try:\n",
    "        data = requests.get(f'https://www.ebi.ac.uk/pdbe/api/pdb/entry/molecules/{code}').json()[code.lower()]\n",
    "        df.loc[i, 'molecule_type'] = data[0]['molecule_type']\n",
    "        df.loc[i, 'seq']      = data[0]['sequence']    \n",
    "    except: \n",
    "        err_prots.append(df.loc[i, 'PDB_code'])\n",
    "        pass \n",
    "\n",
    "print('errors ', err_prots)\n",
    "\n",
    "\n",
    "df.to_csv(\"data/AMP_SEQUENCE_FROM_PDB.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mol2\n",
    "from rdkit import Chem\n",
    "import os\n",
    "\n",
    "path = 'data/AMP/2020'\n",
    "\n",
    "for x in os.listdir(path):\n",
    "    if x.endswith(\".mol2\"):\n",
    "        mol2_file = path + '/' + x\n",
    "        print (mol2_file)\n",
    "        m = Chem.rdmolfiles.MolFromMol2File(mol2_file)\n",
    "        smi_isomeric  = Chem.MolToSmiles(m)\n",
    "        print(smi_isomeric) \n",
    "        #print(smi_canonical)\n",
    "\n",
    "#print(smi_isomeric_sdf) \n",
    "#print(smi_canonical_sdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SDF\n",
    "sdf_file ='data/3i4y_ligand.sdf'\n",
    "mols = [mol for mol in Chem.SDMolSupplier(sdf_file)]\n",
    "for mol in mols:\n",
    "    smi = Chem.MolToSmiles(mol)\n",
    "    name = mol.GetProp(\"_Name\")\n",
    "    print(smi)\n",
    "    print(name)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input a compound in SMILES format where not exist in training or validation or test dataset\n",
    "#1. create the csv file with couple new coumpound and targets (previous procedure)\n",
    "#2. predict affinity for all couples\n",
    "#3. order by affinity desc\n",
    "# test for compound 13-mer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*************************  WORKING ****************************************\n",
    "#                          HYBRID\n",
    "# input is the 256 from embeding\n",
    "# mporei na thelei  dilaSize [1,2,4,8,16]\n",
    "#sto dilation DEN xreiazetai maxpool\n",
    "#1 . na prosthew kai ta apla stoixeia\n",
    "#2. na auxisw ta epipeda\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "import math\n",
    "import pennylane as qml\n",
    "from functools import partial\n",
    "import pennylane.numpy as np\n",
    "\n",
    "\n",
    "#mporei na perasei san parameteros ston init ths klasis\n",
    "n_qubits  = 8\n",
    "n_layers  = 3 # 3 gia ola 1 gia block to paradeigma  PennyLane_01 eixa 6\n",
    "n_features = 256\n",
    "#n_angle_encoding = math.ceil(((n_features)/n_qubits)/2) #dense angle\n",
    "n_angle_encoding = math.ceil(((n_features)/n_qubits)) #angle\n",
    "n_blocks = 2\n",
    "\n",
    "# default.qubit RUN ALL EXPERIMENTS**********\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits )\n",
    "#dev = qml.device(\"lightning.qubit\", wires=n_qubits )  # NOT USED*************\n",
    "\n",
    "#   NOISE SIMULATORS************************************8\n",
    "# Describe noise\n",
    "#noise_gate = qml.AmplitudeDamping\n",
    "#noise_gate = qml.DepolarizingChannel\n",
    "#noise_strength = 0.1\n",
    "\n",
    "# Load devices\n",
    "#dev_ideal = qml.device(\"default.qubit, wires=n_qubits)  # the ideal device with no noise\n",
    "#dev_noisy = qml.transforms.insert(noise_gate, noise_strength)(dev_ideal) # the noisy device with noise\n",
    "#dev_noisy = qml.transforms.insert(dev_ideal, noise_gate, noise_strength, position=\"all\")\n",
    "\n",
    "\n",
    "\n",
    "#  *********************************************\n",
    "# NOT USED *********************************\n",
    "#   *********************************************\n",
    "@qml.qnode(dev)\n",
    "def quantum_state(fetaures, n_qubits):\n",
    "     qml.AmplitudeEmbedding(fetaures, wires=range(n_qubits))\n",
    "\n",
    "def layer(layer_weights, n_qubits):\n",
    "    for i  in range(n_qubits):\n",
    "        qml.Rot(*layer_weights[i], wires=i)\n",
    "\n",
    "    for i in range(n_qubits):\n",
    "        qml.CNOT(wires=[i, (i + 1) % n_qubits])      \n",
    "     \n",
    "\n",
    "def circuit_old(weights, features, n_qubits):\n",
    "    quantum_state(features, n_qubits)\n",
    "\n",
    "    for W in weights:\n",
    "        layer(W, n_qubits)    \n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def circuit_amplitude(inputs, weights):                 #inputs  256 (batch size) x 256  output previous layer\n",
    "\n",
    "    #print('weights.shape', weights.shape)\n",
    "    #print('weights', weights)\n",
    "\n",
    "    qml.AmplitudeEmbedding(inputs, wires=range(n_qubits), normalize=True )\n",
    "\n",
    "    #print('inputs.shape', inputs.shape)\n",
    "    #print('inputs', inputs)\n",
    "\n",
    "    for W in weights: #->  LAYER  ************mipws prepei na ginei    for l in range(n_layers): einai to layer\n",
    "        for i in range(n_qubits):\n",
    "            qml.Rot(*W[i], wires=i)                                   #qml.Rot(weights[l, i + (n_qubits - 1)], wires=i) \n",
    "\n",
    "        for i in range(n_qubits):\n",
    "            qml.CNOT(wires=[i, (i + 1) % n_qubits])\n",
    "\n",
    "    # prosthesa CNOT  me 2 orisma to qubit 1 22/4/2024\n",
    "    for i in range(n_qubits-1):\n",
    "         qml.CNOT(wires=[ (i + 1) % n_qubits, 0])\n",
    "         \n",
    "    return  qml.expval(qml.PauliZ(0))\n",
    "\n",
    "\n",
    "#  *********************************************\n",
    "#  **********************************************\n",
    "#  *********************************************\n",
    "\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def circuit_amplitude_qml(inputs, weights):\n",
    "    #print('inputs.shape', inputs.shape)\n",
    "    #print('inputs', inputs)\n",
    "\n",
    "    qml.AmplitudeEmbedding(inputs, wires=range(n_qubits), normalize=True)\n",
    "    qml.StronglyEntanglingLayers(weights, wires = range (n_qubits), ranges=[1,1,1])   \n",
    "\n",
    "    # prosthesa CNOT  me 2 orisma to qubit 1 22/4/2024\n",
    "    for i in range(n_qubits-1):\n",
    "         qml.CNOT(wires=[ (i + 1) % n_qubits, 0])\n",
    "\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "\n",
    "# einai isodinamo me to panw mono poy to cnot = range[1,2,3] ενώ το πάνω είναι range[1,1,1]\n",
    "@qml.qnode(dev)\n",
    "def circuit_angle_qml(inputs, weights):\n",
    "    #print('inputs.shape', inputs.shape)\n",
    "    #print('inputs', inputs)\n",
    "    for i in range(n_angle_encoding):\n",
    "        qml.AngleEmbedding(math.pi * inputs[:,i*n_qubits:i*n_qubits+n_qubits], wires=range(n_qubits))    #rotation='X'\n",
    "        qml.StronglyEntanglingLayers(weights[i], wires = range (n_qubits), ranges=[1,1,1])   \n",
    "\n",
    "    # prosthesa CNOT  me 2 orisma to qubit 1 22/4/2024\n",
    "    for i in range(n_qubits-1):\n",
    "         qml.CNOT(wires=[ (i + 1) % n_qubits, 0])\n",
    "\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "\n",
    "# ta dedomena apo ta klasika layer prepei na  ginoun normalize\n",
    "#@qml.qnode(dev)\n",
    "@qml.qnode(dev)\n",
    "def circuit_dense_angle(inputs, weights):                 #inputs  256 (batch size) x 256  output previous layer\n",
    "    \n",
    "    #print('inputs.shape', inputs.shape)\n",
    "   # print('inputs', inputs)\n",
    "\n",
    "    #print('weights.shape', weights.shape)\n",
    "    #print('weights', weights)\n",
    "    \n",
    "\n",
    "    #2,3,8,3\n",
    "   # for k in range(inputs.shape[0])\n",
    "    for j in range(n_angle_encoding):               \n",
    "        #print('j' , j)\n",
    "        for i in range(n_qubits):                               #eencoding\n",
    "            qml.RX(math.pi *  inputs[:, i*2   + n_qubits *j] ,  wires=i)      #dense endocoding kai oxi aplo\n",
    "            qml.RY(math.pi *  inputs[:, i*2+1 + n_qubits *j],   wires=i)\n",
    "\n",
    "            #for W in weights[j]:                #LAYER\n",
    "            #    for i in range(n_qubits):\n",
    "            #        qml.Rot(*W[i], wires=i)                                   #qml.Rot(weights[l, i + (n_qubits - 1)], wires=i) \n",
    "\n",
    "            #    for i in range(n_qubits):\n",
    "            #        qml.CNOT(wires=[i, (i + 1) % n_qubits])\n",
    "        \n",
    "        qml.StronglyEntanglingLayers(weights[j], wires = range ( n_qubits ), ranges=[1,1,1])   \n",
    "\n",
    "        # prosthesa CNOT  me 2 orisma to qubit 1 22/4/2024\n",
    "\n",
    "    for i in range(n_qubits-1):\n",
    "         qml.CNOT(wires=[ (i + 1) % n_qubits, 0])\n",
    "            \n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "\n",
    "#########   ALLAXA STO embed_dim 256 -> 128\n",
    "class HQ_Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim=128,  n_filters=16, output_dim=128, dropout=0.2,  n_output=1, dilaSize=1, normalize=True):\n",
    "        \n",
    "        \n",
    "        super(HQ_Net, self).__init__()\n",
    "\n",
    "        self.embed_smile = nn.Embedding(65, embed_dim)\n",
    "        self.embed_prot = nn.Embedding(26, embed_dim)\n",
    "        \n",
    "        #smiles \n",
    "        self.smiles = nn.Sequential(\n",
    "            nn.Conv1d(in_channels= embed_dim, out_channels= embed_dim, kernel_size=3,padding=dilaSize, dilation=dilaSize),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= embed_dim , out_channels= embed_dim, kernel_size=3,padding=dilaSize * 2 ,dilation=dilaSize * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= embed_dim , out_channels= embed_dim, kernel_size=3,padding=dilaSize * 4 ,dilation=dilaSize * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= embed_dim , out_channels= embed_dim, kernel_size=3,padding=dilaSize * 8 ,dilation=dilaSize * 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= embed_dim , out_channels= embed_dim, kernel_size=3,padding=dilaSize * 16,dilation=dilaSize * 16),\n",
    "            #nn.ReLU(), 7/4/2024 gia na epistrepsei 0 eww 1\n",
    "            nn.Sigmoid(), \n",
    "            nn.AdaptiveMaxPool1d(1)                        \n",
    "        )\n",
    "            \n",
    "        #proteins sequence\n",
    "        self.proteins = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=embed_dim, out_channels=embed_dim, kernel_size=3,padding=dilaSize ,dilation=dilaSize),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= embed_dim  , out_channels=embed_dim, kernel_size=3,padding=dilaSize *2 ,dilation=dilaSize *2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= embed_dim , out_channels= embed_dim, kernel_size=3,padding=dilaSize * 4 ,dilation=dilaSize * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= embed_dim , out_channels= embed_dim, kernel_size=3,padding=dilaSize * 8 ,dilation=dilaSize * 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= embed_dim , out_channels= embed_dim, kernel_size=3,padding=dilaSize * 16,dilation=dilaSize * 16),\n",
    "            #nn.ReLU(), 7/4/2024 gia na epistrepsei 0 eww 1\n",
    "            nn.Sigmoid(),   \n",
    "            nn.AdaptiveMaxPool1d(1),\n",
    "          )\n",
    "        \n",
    "\n",
    "        #self.smiles_descriptors =  nn.Sequential(\n",
    "        #     nn.Linear(6 , 6),\n",
    "        #     nn.ReLU()\n",
    "        # )\n",
    "            \n",
    "        #self.predict = nn.Sequential(\n",
    "        #                            #nn.Linear(2 * embed_dim + 6, 1024), #6 smiles descriptors                                    #nn.Linear(2 * embed_dim + 6, 1024), #6 smiles descriptors\n",
    "        #                            nn.Linear(2 * embed_dim , 1024), #6 smiles descriptors\n",
    "        #                            nn.ReLU(),\n",
    "        #                            nn.Dropout(dropout),\n",
    "        #                            nn.Linear(1024, 512),\n",
    "        #                            nn.ReLU(),\n",
    "         #                           nn.Dropout(dropout),\n",
    "         #                           nn.Linear(512, n_output)\n",
    "         #                           )\n",
    "      \n",
    "        #for amplitube\n",
    "        #weight_shapes = {\"weights\": ( n_layers, n_qubits, 3)}\n",
    "\n",
    "        #for amplitube block\n",
    "        #weight_shapes = {\"weights\": ( n_blocks,  n_layers, n_qubits, 3)}\n",
    "\n",
    "\n",
    "        # for angle\n",
    "        weight_shapes = {\"weights\": (n_angle_encoding, n_layers, n_qubits, 3)} # 3 einai gia to rot\n",
    "\n",
    "       \n",
    "        #qnode = qml.QNode(circuit_amplitude_qml, dev, interface='torch', diff_method='backprop')\n",
    "        #qnode = qml.QNode(circuit_angle_qml, dev, interface='torch', diff_method='backprop')\n",
    "        qnode = qml.QNode( circuit_angle_qml, dev, interface='torch', diff_method='backprop')\n",
    "\n",
    "                \n",
    "       # self.predict_q = qml.qnn.TorchLayer(qnode, weight_shapes, init_method=nn.init.uniform_) #Semantic\n",
    "        \n",
    "        self.predict_q = qml.qnn.TorchLayer(qnode, weight_shapes) \n",
    "        \n",
    "\n",
    "        #nn.init.uniform_(self.predict_q.weight.data, a=0.0, b=0.01, generator=None)\n",
    "  \n",
    "    def forward(self, smi_in, seq_in,  smi_desc):#65,26\n",
    "\n",
    "        #print('1. smi_in', smi_in.shape)        \n",
    "        #print('1. seq_in', seq_in.shape)\n",
    "        \n",
    "        embedded_smi  = self.embed_smile(smi_in) \n",
    "        embedded_seq  = self.embed_prot(seq_in)\n",
    "        \n",
    "        #smi_desc =  self.smiles_descriptors(smi_desc) \n",
    "        \n",
    "        #print('2. embedded_smi.shape', embedded_smi.shape)        \n",
    "        #print('2. embedded_smi.shape', embedded_seq.shape)\n",
    "        \n",
    "        smi = self.smiles(embedded_smi.transpose(1,2))\n",
    "        seq = self.proteins(embedded_seq.transpose(1,2))\n",
    "        \n",
    "        #print('3. smi.shape', smi.shape)        \n",
    "        #print('3. seq.shape', seq.shape)\n",
    "        \n",
    "        #Squeeze = Reduce('b c t -> b c', 'max')(smi) #einai swsto?? diafora me Test_3\n",
    "        #seq = Reduce('b c t -> b c', 'max')(seq) #einai swsto??\n",
    "        \n",
    "        smi = smi.squeeze()\n",
    "        seq = seq.squeeze()\n",
    "\n",
    "        #print('4. smi.shape', smi.shape)        \n",
    "        #print('4. seq.shape', seq.shape)\n",
    "        \n",
    "        #smi = smi.view(-1, 71 * 128 * 26) #32 = n_filters * 2\n",
    "        #seq = seq.view(-1, 71 * 128 * 26) #32 = n_filters * 2\n",
    "        \n",
    "\n",
    "        # concat\n",
    "        #smi_seq = torch.cat((smi, seq, smi_desc),1)\n",
    "        #print('smi.shape', smi.shape)\n",
    "        #print('smi', smi )\n",
    "        \n",
    "        smi_seq =torch.cat((smi, seq) , 1)  # ΔΕΝ ΒΑΖΩ ΤΑ 6 χαρακτηριστικά\n",
    "\n",
    "        #print('5.smi_seq ', smi_seq.shape)\n",
    "        \n",
    "        #PROSOXI GINETAI NORMALIZE EINAI MONO GIA AMPLIRUDE??????????????????????\n",
    "        #smi_seq = smi_seq / np.linalg.norm(smi_seq)\n",
    "\n",
    "        #shape = qml.math.shape(smi_seq)\n",
    "        #norm = qml.math.sum(qml.math.abs(smi_seq) ** 2, axis=-1)\n",
    "        #smi_seq = smi_seq / qml.math.reshape(qml.math.sqrt(norm), (*shape[:-1], 1))\n",
    "        \n",
    "\n",
    "        #out = self.predict(smi_seq)\n",
    "        out = self.predict_q(smi_seq)            #quantum\n",
    "        \n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Μαρία\\AppData\\Local\\Temp\\ipykernel_13392\\1622923308.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(file_path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HQ_Net(\n",
       "  (embed_smile): Embedding(65, 128)\n",
       "  (embed_prot): Embedding(26, 128)\n",
       "  (smiles): Sequential(\n",
       "    (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "    (3): ReLU()\n",
       "    (4): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "    (5): ReLU()\n",
       "    (6): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
       "    (7): ReLU()\n",
       "    (8): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,))\n",
       "    (9): Sigmoid()\n",
       "    (10): AdaptiveMaxPool1d(output_size=1)\n",
       "  )\n",
       "  (proteins): Sequential(\n",
       "    (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "    (3): ReLU()\n",
       "    (4): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "    (5): ReLU()\n",
       "    (6): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
       "    (7): ReLU()\n",
       "    (8): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,))\n",
       "    (9): Sigmoid()\n",
       "    (10): AdaptiveMaxPool1d(output_size=1)\n",
       "  )\n",
       "  (predict_q): <Quantum Torch Layer: func=circuit_angle_qml>\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#file_path = 'models/HQ_4_qubit_dense_angle.pt'\n",
    "file_path = 'models/HQ_8_qubit_angle.pt'\n",
    "\n",
    "model = HQ_Net()\n",
    "model.load_state_dict(torch.load(file_path))\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b7b3b5cc8b4cb7aa0ebb3eefbf4b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32448918 0.34110376 0.2893315  0.33365691 0.25950497 0.289381\n",
      " 0.31617552 0.32676232 0.32087007 0.31819445 0.32111076 0.29779857\n",
      " 0.26789835 0.31664518 0.31266719 0.34390098 0.26368973 0.32454661\n",
      " 0.30962488 0.32381776 0.29514199 0.32858428 0.29902297 0.32692522\n",
      " 0.28144416 0.27950981 0.32877833 0.33919424 0.29251918 0.3388814 ]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "269eeeafd83343a0a0c9d912a2ee19fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32041615 0.3406339  0.28499755 0.33325028 0.25950497 0.289381\n",
      " 0.31797197 0.32921922 0.31363738 0.31819445 0.32479626 0.30792978\n",
      " 0.26789835 0.32434723 0.31390736 0.35254797 0.26386014 0.33773538\n",
      " 0.31475815 0.32525966 0.29640913 0.32256579 0.30435574 0.32692522\n",
      " 0.28470322 0.28470322 0.32738286 0.34615391 0.30389899 0.32879233]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54be43771dc146b3bed02aa67044c7e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36764964 0.38419247 0.33026871 0.37589559 0.30500656 0.33407617\n",
      " 0.35941458 0.36978585 0.36364684 0.36211056 0.36464661 0.34158909\n",
      " 0.31296951 0.35980529 0.35795823 0.38602564 0.30891326 0.36877349\n",
      " 0.35422122 0.36737448 0.33978379 0.37102294 0.34412584 0.37077102\n",
      " 0.32200101 0.32019889 0.37161058 0.37967303 0.33766755 0.38242185]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ff4eeb1c614d3a9d96b776b0fec5b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36365426 0.3837842  0.32613871 0.3754355  0.30500656 0.33407617\n",
      " 0.3616097  0.37211636 0.35679039 0.36211056 0.36880589 0.34987587\n",
      " 0.31296951 0.3671883  0.35639399 0.39370355 0.30913687 0.38067922\n",
      " 0.35821208 0.36913848 0.33967626 0.36691767 0.34890836 0.37077102\n",
      " 0.32587647 0.32587647 0.36994699 0.38637164 0.34838429 0.37221971]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f21c78c5a6a4203bbc48bddb3d0edd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.37614852 0.39258289 0.33935845 0.38352454 0.31330431 0.34139237\n",
      " 0.3674202  0.37728074 0.37230685 0.3690736  0.37193379 0.34930202\n",
      " 0.32131714 0.36827272 0.36472303 0.39422476 0.31715333 0.37587169\n",
      " 0.36168751 0.37578583 0.34825194 0.37839112 0.35112706 0.37826833\n",
      " 0.33101845 0.32960489 0.37990943 0.38876984 0.34525126 0.39051896]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3242bf2651d4f37b13d079e8d5674c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.37146857 0.39217988 0.33535638 0.38304397 0.31330431 0.34139237\n",
      " 0.36986443 0.37969428 0.36531976 0.3690736  0.37634006 0.35866302\n",
      " 0.32131714 0.37607858 0.36494058 0.4028599  0.31737021 0.38879958\n",
      " 0.3662571  0.37706035 0.34862861 0.37351981 0.35614252 0.37826833\n",
      " 0.33506939 0.33506939 0.37878853 0.39555424 0.3561506  0.37991557]\n"
     ]
    }
   ],
   "source": [
    "from dataset import MyDataset_predict\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "batch_size = 256\n",
    "data_path = 'data/' \n",
    "#format files ,PDB code,smiles,molecule_type,seq\n",
    "file_names = ['AMP_SMILES_FROM_FILE',\n",
    "              'AMP_SMILES_FROM_FILE_SEQ_FROM_PDB',\n",
    "              'AMP_SMILES_FROM_PUBCHEM',\n",
    "              'AMP_SMILES_FROM_PUBCHEM_SEQ_FROM_PDB',\n",
    "              'AMP_SMILES_FROM_RCS',\n",
    "              'AMP_SMILES_FROM_RCS_SEQ_FROM_PDB']\n",
    "\n",
    "for file_name in  file_names:\n",
    "    max_seq_len = 1000\n",
    "    max_smi_len = 160\n",
    "    \n",
    "    test_loader =  DataLoader(MyDataset_predict(data_path, file_name, max_seq_len, max_smi_len),\n",
    "                                             batch_size=batch_size,\n",
    "                                             pin_memory=True,\n",
    "                                             num_workers=4,\n",
    "                                             shuffle= False)\n",
    "    \n",
    "    cuda_name = \"cuda:0\"\n",
    "    device = torch.device(cuda_name if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        for idx, (*x, y) in tqdm(enumerate(test_loader), disable=False, total=len(test_loader)):\n",
    "            for i in range(len(x)):\n",
    "                x[i] = x[i].to(device)\n",
    "       \n",
    "            y_hat = model(*x)\n",
    "            outputs.append(y_hat.cpu().numpy().reshape(-1))\n",
    "    \n",
    "    \n",
    "    outputs = np.concatenate(outputs).reshape(-1)\n",
    "    \n",
    "    \n",
    "    print(outputs)\n",
    "    \n",
    "    \n",
    "    # convert array into dataframe \n",
    "    df_o = pd.DataFrame(outputs) \n",
    "    output_path = Path('predict/')\n",
    "    df_o.to_csv(output_path / f\"{file_name}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
